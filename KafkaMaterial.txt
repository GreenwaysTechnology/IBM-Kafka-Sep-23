				Kafka
.....................................................................................
Learning Track:
...............

1.Introduction to kafka 
2.Kafka Architecture
3.Kafka Programming using cli
4.Producers and Consumers
5.Kafka Connect
6.Kafka Streams using KsqlDB

Note: we dont use any programming language to connect to produce and consume kafka messages.



What is Kafka?

Data is every thing;
///////////////////

Every Enterprise is powered by data.

We take information in, analyze it, manipulate it and creates more as output.

Every application creates data, whether it is log messages,metrics,user activity,out going messages, or something else.

Every byte of data has a story to tell, something of imporatance that will inform the next thing to be done.

In order to know what that is, we need to get the data from where it is created to where it can be analyzed.

We see this every day on websites like amazon,youtube,facebook, where our "clicks" on on items of interest to use are turned into recommmendations that are shown to us litte later.

The faster we can do this, the more agile and resonsive our organizations can be.
The less effort we spend on moving data around, the more we can focus on the core business at hand.

....................................................................................
			Publish and Subscribe Messaging (data):
...................................................................................

Before discussing the Apache Kafka , it is imporant for us to understand the concept of pub/sub messaging and why it is important.

Pub and sub messaging is  a pattern that is characterized by the sender(publisher) of a piece of data (message) not spcificially directing to a reciver, Instead, the publisher classifies the message somewhat, and that receiver(subscriber) subscribes to receive certain of classes of messages.

Pub /Sub systems often have a broker, a central point where messages are published , to facilite this.

.....................................................................................
			  How enterprise systems handles data
			             (Pre Kafka)
....................................................................................


Many use cases for pub/sub starts with same way.

   With a simple message queue or interprocess communication channel

for eg, you create an application that needs to send montioring information somewhere. How do you send?

You write monitoring message in a direct connection from your application to an application that displays your metrics on a dashboard, push metrics over that connection.

let us say, you have systems, that system has two servers - frontend server,
back end server

both server sends metrics data to metrics server

		 FrontEnd                 BackEnd Server
		  Server
		    |				|
		    |				|
		--------------------------------------
				|
			    Mertics Server

If your server is running in clustered env

		FrontEnd               FrontEnd Server
		  Server
		    |				|
		    |				|
		--------------------------------------
				|
			    Mertics Server


		 Front              Backend Server
		  Server
		    |				|
		    |				|
		--------------------------------------
				|
			    Mertics Server

A single , direct metric server irresptive of how many backend and front end server

This looks a simple soultion to a problem that works when you are going to getting started with monitoring.

After long time,you decide you would like to analyze your metircs over a longer term,
that does not work very well in dashboard.

When you introduce new service in your biz and where you have to introduce server,
Now you have three more apps, that generating metrics data ,then metrics server need connect directly , recive,store,anaylze

..............................................................................
			Many Metrics publisher, using direct connections
.....................................................................................

 FrontEnd server  Database Server  Chat server  Mail Server PaymentServer
         |            |                |           |            |
-----------------------------------------------------------------------
                                |
			   publish metrics
				|
			    Metric Server


Here all publisher are publishing "directly" mertics to Metrics Servers.

       "What if i want to store front data ,database data,back end data separatly"

....................................................................................
			Loosly Coupled Metric publisher and Server
		         Introduction of Pub/Sub Messing System
...................................................................................

 FrontEnd server  Database Server  Chat server  Mail Server PaymentServer
         |            |                |           |            |
-----------------------------------------------------------------------
                                |
			   publish metrics
				 |
			      Metrics
			      Sub/Pub
				|
			    Metric Server

Every Pub sub system is going to store messages inside "Queue" , the basic data storage model.
In the above system we have only one /Single /Individual Queue System.

Image one of your coworkers has been doing similar work with log messages, another has been working on tracking user behavior on the frontend website and providing that information to developers who are working on machine learning,
As well as creating some reports for management.

...................................................................................
			 Multi Pub Sub Systems
...................................................................................

FrontEnd server  Database Server  Chat server  Mail Server PaymentServer 
         |            |                |           |            |
-----------------------------------------------------------------------
                                |

Metrics     Logging              Tracking
Pub/Sub     Pub/Sub              Pub/Sub
  |           |                    |
Metric    ---------             ----------       
Server    |       |                |
        Secuirty Log Search      MachingLearning 
	Analysis Server		 and AI server
         


Now at last , we have refactored our system, but there is lot of "Duplication"
Your company is maintaining multiple systems for queuing data, all of which have their own individual bugs and limitations.
You will have more systems in future it will come.
....................................................................................
			Birth of Kafka :Entering into Kafka
....................................................................................	
Apache Kafka is pub/sub messaging system designed to solve the above problem.
Instead of having multiple  Queue System, we can have only one System where we receive message,organize the message,store,process,and produce the report.

Traditional Messaging Systems:
..............................

Traditional Messaging systems are built based on the standards like "AMQP" protocal.
Any pub/sub messaging product like rabbit mq is built on the standards only.

According to the AMQP Standards.
 
1.Messages are stored in a queue
2.Queue stores messages which is tranisent by default. if you want you can persit in disk.
3.The messages can be altered(update,delete)
4.The messages are deleted once it is consumed
.....................................................................................

	"Kafka was not designed based on Traditional Messaging System"
	 "Kafka was not designed based on AMQP Protocal Specification"

Kafka inspired from "Logging System" or Loggers to store messages, instead of storing message in traditional messaging systems.

 		"Kafka was designed based on  Loggers"

What is Log?
   Tracking activites of an application,store those activites in "memory or in  a disk file" in order to analyze them in furture.

If you are developer, you encounter loggers every day in your development cycle.

Logs gives complete information about the system which is running.

Log gives just information about "what just happened or happing" in your system for eg
some warings,some info,some bugs, some tracking , some tracing..........

Logs :
2016-06-16 17:02:13 TRACE Trace log message
2016-06-16 17:02:13 DEBUG Debug log message
2016-06-16 17:02:13 INFO  Info log message
2016-06-16 17:02:13 ERROR Error log message

.....................................................................................
			 Log structure and its characteristics
.....................................................................................

Log information is stored in a file called "Log file" - system.log
Log file is used for future analytics such as debugging,finding warnings,errors...


What is difference between "normal files" and log files?

=>Log files are "append only", you cant add any new entry in between, which makes the file "immutable" - cant be edited or read only.

=>Normal files are based on "Edit Mode" or Replace mode
    Files are edited or replaced later.

  		    "Kafka is just based on Log System"
		       Kafka is just Logger System.

   Since kafka is logger system is it same as "Slf4j,log4j" Kind of loggers.

Some what yes, but Kafka is more beyond that....

	    Kafka is not based on "traditional log files" 

Kafka is fundmentally based on "Commit Logs"

What is commit log?
    "In data management platforms, a commit is making set of tenative changes permanent".
    "Making the end of a transaction and providing Durablity to ACID transactions"
  The record of commits is called "Commit log"

What Kafka is going to record into commit log?
     Kafka was designed to store informations(data).

What type of information?
  Any type of information we can store into commit log.
.....................................................................................
			   Event
....................................................................................
The term “event” shows up in a lot of different Apache Kafka® arenas. 

There’s “event-driven design,” “event sourcing,” “designing events,” and “event streaming.”

What is an event, and what is the difference between the role an event has to play in each of these contexts? 

What are events?
  We can speak broadly, maybe even a little philosophically, about what events are. 

Events are “things that happen,” or sometimes, they are otherwise defined as representations of facts.

All data is, in a way, a result of humans trying to grok events. At the same time.

Let’s get concrete: Events that might affect real-time data pipelines and applications, including things like Pinterest saves, USPS address changes, ship coordinate updates, and credit card transactions. 

  An Event is any type of action,incident,or change are "happening" or "just happened"

for eg:
  Now i am typing,Now i am teaching - happening
  Just i had coffee,Just i received mail, just i clicked a link, just i searched product - happened.

 "An Event is just remainder or notification of  your happenings or happened"

Designing events: Designing events means carefully choosing the conceptual model for your event representation based on the role the event has to play.

Event streaming: You can stream events in real time, as well as aggregate, filter, and join multiple streams. This process is called event streaming. 

Event-driven design: Event-driven design means designing your architecture in a manner that’s informed by the reactive nature of events. 

...................................................................................
		     Event Driven Architecture(Software system)
....................................................................................

The Software system can track what is happening, just happended , stores into a file called commit log, later that commit log can be replayed to process those events to produce various reports

			    FronEnd Server
				|
			  What is happening or happened
			 (User has clicked  "iphone 15 link") - event
				|
			   store userclick event into log file
				|
			     Kafka 
				|
			    events.log
				17-07-2023 3:48:59  iphone 15 link
				17-07-2023 3:49:58  dell lap top link



Let us imagine, You have mobile apps, which tracks your locations where ever you move, those locations are recorded into a file by "Event Driven System"(Kafka).
Based on those data , you can get report like that where were you at morning,afternoon,evening...

Eg:
 Today stock price is $40 at 10Am
 I met my friend yesterday at east coast road
 Made payment of $500 to Ramesh

Imgaine i need  somebody or somthing should record every activity of my life from the early moring when i get up and till bed.

  There is a system to record every events of your life that is called 
			      Kafka

	 Kafka is Event Processing Software , which stores and process events


Kafka Event structure:
  Kafka events are key value pairs

event = {
 key: "nerve_signal" 
 value: "beta: 12Hz, gamma: 8Hz"
 timestamp: "1979 8:52:32 AM GMT-07:00"
}

.....................................................................................
			Kafka Basic  Architecture
.....................................................................................

How kafka has been implemented?

   "Kafka is a software"
   "Kafka is a file(Commit log file) processing software
   "Kafka is written in java and scala" - Kafka is just java application
   "In order to run Kafka we need JVM"

How event is represented into kafka?

	Event is just a message.
        Every message has its own arch.
        In Kafka the Event/Message is called as "Record".
		Event(Record)


Event Contains Two things:
..........................
1.What happened/Happing - Name of the Event
2.State - Data

State:
......
  The state is nothing but data.

State Representation:

 In General state(data) is stored in relational databases "as table"
 A table represents the state of something like 
    User - id,name,email,password,city

Since User data can be stored and proceed into tables.

Can we store events into table?
   Events also has state like things(user,customer,product) in real time.

We can but not all types of events into table.
.....................................................................................
			   Event driven data Modeling : Modern Data Modeling
.....................................................................................
     Generally domains are modeled based on "Things(Customer,Order,Payment) first"
	  Now a days People started thinking based on Events first
          Instead of storing things into database , we store events

Events also has some state like "Things"

   "Events has some description of what happened with it", but Primary idea is that          event is indication in time that thing took place".

How to store events?
   Logs - Log is structured and the sequence  of the evnets occured in the method calls.

According to Kafka Official Definition:

	"Apache Kafka is an open source distributed streaming system used for stream 	processing, real-data time pipelines, and data integration at scale"
...................................................................................
....................................................................................
			 kafka Distribution - Kafka Setup
...................................................................................

Kafka was orginally created by "Jay kreps,Neha,Jun Rao" at Linkedin to solve the problems of distributed "Pub/Sub" Message system.

Once the Kafka was ready, Kafka Creators wanted to open source, who released the Kafka under "Apache license" early 2011.

After Kafka relase it become very popular, later Jay ,Neha ,Jun Rao started the company called "Confluent".

Confluent then took Apache Kakfa as a core and who built various production ready tools, support, cloud integration


Kafka distribution:
 Kafka is available in two distribution

1.Apache Kafka
   It is open source version of kafka 

2.Confluent Kafka
   It is abstraction of apache kafka, Commericial version of apache kafka


Apache kafka vs confluent kafka
https://www.confluent.io/apache-kafka-vs-confluent/

Platforms:

Kafka can be installed any platform

1.Bare metal machines
  Kafka is available for all operating system.

1.Windows - may be good for basic use cases
2.Linux - recommended for advanced use cases
3.mac - recommended for advanced use cases

2.VM env
  You  can setup kafka on any industry standard VMS - oracle virtual box

3.Container based distributed: - docker and kubernetes
   It is recommened in development env and also can be used in prod

We are going to setup:

Apache Kafka | confluent Kafka
1.Linux - bare metal machine
2.Docker - Container

Lab setup:

1.setup linux:
..............

We are going to use windows 10 or 11 WSL 2 Feature to install linux.

https://www.confluent.io/blog/set-up-and-run-kafka-on-windows-linux-wsl-2/

Linux: Ubuntu 20.x

sudo apt get-update

sudo apt get-upgrade

Windows Terminal Software:
  Multi window terminal used for multiple windows opening in single window.

2.Install JDK 11

sudo apt install openjdk-11-jdk -y

3.Verification of jdk installation
java --version
openjdk 11.0.20.1 2023-08-24
OpenJDK Runtime Environment (build 11.0.20.1+1-post-Ubuntu-0ubuntu120.04)
OpenJDK 64-Bit Server VM (build 11.0.20.1+1-post-Ubuntu-0ubuntu120.04, mixed mode, sharing)


4.Setting up Kafka:


1.Apache Kafka -https://kafka.apache.org/
  =>Source distribution
	-you can build from the source
  =>Binary distribution
        -you can download already built folder


Download link:
https://downloads.apache.org/kafka
https://archive.apache.org/dist/kafka


wget https://downloads.apache.org/kafka/3.5.0/kafka_2.13-3.5.1.tgz

tar -xzf  kafka_2.13-3.5.1.tgz



/kafka_2.13-3.5.1$ ls -l
total 64
-rw-r--r-- 1 subu subu 14722 Jul 14 22:20 LICENSE
-rw-r--r-- 1 subu subu 28184 Jul 14 22:20 NOTICE
drwxr-xr-x 3 subu subu  4096 Jul 14 22:23 bin
drwxr-xr-x 3 subu subu  4096 Jul 14 22:23 config
drwxr-xr-x 2 subu subu  4096 Sep 21 14:45 libs
drwxr-xr-x 2 subu subu  4096 Jul 14 22:23 licenses
drwxr-xr-x 2 subu subu  4096 Jul 14 22:23 site-docs

Folder structures

libs:
  contains all jar files neccessary to run kafka.
bin;
 contains the shell script files to run kafka servers and all cli
 contains windows folder.

bin/windows
  contains the batch files to run kafka in windows env.

config;
  contains all configuration files related to kafka server,zookeeper.....
...................................................................................
.....................................................................................

			Core concepts of Kafka
.....................................................................................

Broker:
.......
   Since Kafka is a java program which is deployed on JVM,Kafka runs on the JVM Which is process.
   The JVM is other wise called as "Kafka Broker or Kafka Server"

.....................................................................................
			 Types of Kafka Broker
.....................................................................................

Kafka has been designed based on "Distributed Architecture" - By Default Kafka is distributed.

General Characteritics of Disbutributed Architecture:
.....................................................

1.Scalablity
    Running more than one process,hosting the same app. Running the same app on    multiple servers.

Cluster:
  When we scale apps into multiple servers, we need to group them under a single unit.
  Group of machines are called as "cluster"

2.High Availablity:
   if any one server fails in the cluster, clients should not be affected, we need to make our app always available.
   How to make highly available?
      Via cluster

         In kafka we can run "Multiple Brokers" as a cluter.

  Kafka cluster can be in the same machine or across machines in network.
...........
.....................................................................................
			   Cluster Manager
.....................................................................................

In any distributed arch, if machines are running in a cluster or clusters , the cluster need to be mananged.

Who can manage cluster?
   Cluster Manager.

Kafka and cluster Manager:
  Kafka is distributed, runs in a cluster, we need to manage that cluster.

Kafka provides cluster manager
  =>ZooKeeper - It is distributed cluster manager software
  =>KRaft -  it is new cluster manager inside Kafka cluster.

if you run single broker or multiple brokers we need to have cluster manager.

1.Apache Zookeeper:
	ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. 

2.Apache KRaft:
    KRaft is consenus protocal that was introduced to replace ZooKeeper for meta data management

Roles of Cluster Managers:
1.To manage cluster
2.Failures detections and recovery
3.Storing ACL and secrets


Lab 2:
 How to setup Apache Kafka Cluster.
 
Single Broker , Single zookeeper.

Step 1:
 Start zookeeper

Note: before starting zookeeper or broker we need to pass "respective config files" as parameter 

zookeeper:
 Has a config file called config/zookeeper.properties

dataDir=/tmp/zookeeper
    The directory where the snapshot of cluster information is stored.

clientPort=2181
  The Port at which clients connect , 
  who is client? Kafka Broker is client.

Any server if you want to start we need to use "script files" which is inside bin folder

$./bin/zookeeper-server-start.sh  config/zookeeper.properties


Step 2:
  After running zookeeper,we need start broker

in order to start kafka broker we need to supply server.properties file...

./bin/kafka-server-start.sh config/server.properties

broker.id=0
   Each broker is idenfitified uniquely in the cluster.

log.dirs=/tmp/kafka-logs
    Location of event log files....

zookeeper.connect=localhost:2181
     The current broker is connected with zookeeper and its host and port

.....................................................................................
				Topics
....................................................................................
What is Topic?
  There are lot of events, we need to organize them in the system.
  Apach Kafka's most fundamental unit of organization is the topic.

  Topic is just like tables in the relational database.

  As we discussed already, Kafka just stores events in the log files.

  We never writes events into log file directly

 As a developer we capture events, write them into "topic",Kafka writes events into    log file from the topic.

  A topic is log of events,logs are easy to understand

  Topic is just simple datastructure with well known semantics, They are append only.

  When ever you write a  message, it always goes on the end.

  When you read message from the logs by "Seeking offset in the log"

  Logs are fundamental durable things,Traditional Messaging systems have topics and     queues which store messages temporarily to buffer them between source and      designation.

  Since topics are logs , which always permenant.

   You can delete log files not log messages.

  You can store logs as short as to as long as years or even retain message     indefinitely.
.....................................................................................	
			.....................................................................................
			How to create topics
.....................................................................................

In order to create topic, we need somebody to create topic.


Actors In kafka Systems:
........................

1.Producer
   The Producer is a program whose responsability to capture events,and send events to Kafka broker.
   Producer will publish events into topic.
2.Consumer
 The Consumer is a program whose responsability to read events from the topic

Producer and consumers can be written any programming language which supports kafka integration.

Producers can be java program or node.js program or python or c#
Consumer can be java program or node.js program or python or c#

Other than programming languages , Kafka supports cli tools.
.....................................................................................

In order to publish and consume events into and from kafka , we are going to use "Kafka cli tools"

1. kafka topic tool
    Used to create,describe,delete topics
2. Kafka producer tool 
    Used to publish events into topic
3. kafka consumer tool
   Used to consume events from the topic

.....................................................................................				kafka-topics.sh
..................................................................................
Used to create,describe,delete topics

Lab: How to create topic

1.Explore help how to use topics cli tool.

 ./bin/kafka-topics.sh --help
This tool helps to create, delete, describe, or change a topic.
Option                                   Description
------                                   -----------
--alter                                  Alter the number of partitions and
                                           replica assignment. Update the
                                           configuration of an existing topic
                                           via --alter is no longer supported
                                           here (the kafka-configs CLI supports
                                           altering topic configs with a --
                                           bootstrap-server option).
--at-min-isr-partitions                  if set when describing topics, only
                                           show partitions whose isr count is
                                           equal to the configured minimum.
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect
  connect to>                              to.
--command-config <String: command        Property file containing configs to be
  config property file>                    passed to Admin Client. This is used
                                           only with --bootstrap-server option
                                           for describing and altering broker
                                           configs.
--config <String: name=value>            A topic configuration override for the
                                           topic being created or altered. The
                                           following is a list of valid
                                           configurations:
                                                cleanup.policy
                                                compression.type
                                                delete.retention.ms
                                                file.delete.delay.ms
                                                flush.messages
                                                flush.ms
                                                follower.replication.throttled.
                                           replicas
                                                index.interval.bytes
                                                leader.replication.throttled.replicas
                                                local.retention.bytes
                                                local.retention.ms
                                                max.compaction.lag.ms
                                                max.message.bytes
                                                message.downconversion.enable
                                                message.format.version
                                                message.timestamp.difference.max.ms
                                                message.timestamp.type
                                                min.cleanable.dirty.ratio
                                                min.compaction.lag.ms
                                                min.insync.replicas
                                                preallocate
                                                remote.storage.enable
                                                retention.bytes
                                                retention.ms
                                                segment.bytes
                                                segment.index.bytes
                                                segment.jitter.ms
                                                segment.ms
                                                unclean.leader.election.enable
                                         See the Kafka documentation for full
                                           details on the topic configs. It is
                                           supported only in combination with --
                                           create if --bootstrap-server option
                                           is used (the kafka-configs CLI
                                           supports altering topic configs with
                                           a --bootstrap-server option).
--create                                 Create a new topic.
--delete                                 Delete a topic
--delete-config <String: name>           A topic configuration override to be
                                           removed for an existing topic (see
                                           the list of configurations under the
                                           --config option). Not supported with
                                           the --bootstrap-server option.
--describe                               List details for the given topics.
--exclude-internal                       exclude internal topics when running
                                           list or describe command. The
                                           internal topics will be listed by
                                           default
--help                                   Print usage information.
--if-exists                              if set when altering or deleting or
                                           describing topics, the action will
                                           only execute if the topic exists.
--if-not-exists                          if set when creating topics, the
                                           action will only execute if the
                                           topic does not already exist.
--list                                   List all available topics.
--partitions <Integer: # of partitions>  The number of partitions for the topic
                                           being created or altered (WARNING:
                                           If partitions are increased for a
                                           topic that has a key, the partition
                                           logic or ordering of the messages
                                           will be affected). If not supplied
                                           for create, defaults to the cluster
                                           default.
--replica-assignment <String:            A list of manual partition-to-broker
  broker_id_for_part1_replica1 :           assignments for the topic being
  broker_id_for_part1_replica2 ,           created or altered.
  broker_id_for_part2_replica1 :
  broker_id_for_part2_replica2 , ...>
--replication-factor <Integer:           The replication factor for each
  replication factor>                      partition in the topic being
                                           created. If not supplied, defaults
                                           to the cluster default.
--topic <String: topic>                  The topic to create, alter, describe
                                           or delete. It also accepts a regular
                                           expression, except for --create
                                           option. Put topic name in double
                                           quotes and use the '\' prefix to
                                           escape regular expression symbols; e.
                                           g. "test\.topic".
--topic-id <String: topic-id>            The topic-id to describe.This is used
                                           only with --bootstrap-server option
                                           for describing topics.
--topics-with-overrides                  if set when describing topics, only
                                           show topics that have overridden
                                           configs
--unavailable-partitions                 if set when describing topics, only
                                           show partitions whose leader is not
                                           available
--under-min-isr-partitions               if set when describing topics, only
                                           show partitions whose isr count is
                                           less than the configured minimum.
--under-replicated-partitions            if set when describing topics, only
                                           show under replicated partitions
--version                                Display Kafka version.



Create a New Topic:
.....................
./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic todos-topic
Created topic todos-topic.

--bootstrap-server localhost:9092
   =>The broker address 
--create
   create new topic operation
--topic
   options for topic to be created

After creating topic, you can explore in the directory location

/tmp/kafka-logs
	|
  	todos-topic-0
When ever new topics is created ,  a folder is created at low level/disk level

todos-topic-0
    |        |   
 topics Name partition id
..................................................................................
			How to look the topic structure
..................................................................................

--describe --topic todos-topic

./bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic demo-topic

Topic: demo-topic  TopicId: PfLoD1OpSEG51aNFuj_k4Q 
PartitionCount: 1  
ReplicationFactor: 1 
 Configs:
   Topic: demo-topic       Partition: 0    Leader: 0       Replicas: 0     Isr: 0
.....................................................................................
		  How to delete the topic
.....................................................................................

--delete --topic nameofthe topic

./bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic demo-topic

After delete operations is executed, the topic is renamed rather it wont  be deleted from the disk immediatly.

After some time , the folder to be deleted, deletion will take some "Timeout" 

Server logs:

[2023-09-21 16:41:08,987] INFO Log for partition todos-topic-0 is renamed to /tmp/kafka-logs/todos-topic-0.ca0460436ecc442286e40937ac6d5a46-delete and is scheduled for deletion (kafka.log.LogManager)

[2023-09-21 16:42:08,987] INFO [LocalLog partition=todos-topic-0, dir=/tmp/kafka-
logs] Deleting segments as the log has been deleted: LogSegment(baseOffset=0, size=0, lastModifiedTime=1695293959528, largestRecordTimestamp=None) (kafka.log.LocalLog)
[2023-09-21 16:42:08,988] INFO [LocalLog partition=todos-topic-0, dir=/tmp/kafka-logs] Deleting segment files LogSegment(baseOffset=0, size=0, lastModifiedTime=1695293959528, largestRecordTimestamp=None) (kafka.log.LocalLog$)

[2023-09-21 16:42:08,989] INFO Deleted log /tmp/kafka-logs/todos-topic-0.ca0460436ecc442286e40937ac6d5a46-delete/00000000000000000000.log.deleted. (kafka.log.LogSegment)

[2023-09-21 16:42:08,990] INFO Deleted offset index /tmp/kafka-logs/todos-topic-0.ca0460436ecc442286e40937ac6d5a46-delete/00000000000000000000.index.deleted. (kafka.log.LogSegment)

[2023-09-21 16:42:08,990] INFO Deleted time index /tmp/kafka-logs/todos-topic-0.ca0460436ecc442286e40937ac6d5a46-delete/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment)

[2023-09-21 16:42:08,990] INFO Deleted log for partition todos-topic-0 in /tmp/kafka-logs/todos-topic-0.ca0460436ecc442286e40937ac6d5a46-delete. (kafka.log.LogManager)
.....................................................................................
		How to list the no of topics in the current broker
.....................................................................................
/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
todos-topic

.....................................................................................
		 How to publish event(message,record) into kafka topic
....................................................................................

In order to publish message/event/record we need publisher, publisher could be any program, we are going to use "cli" to publish message.

./bin/kafka-console-producer.sh --help

This tool helps to read data from standard input and publish it to Kafka.

Option                                   Description
------                                   -----------
--batch-size <Integer: size>             Number of messages to send in a single
                                           batch if they are not being sent
                                           synchronously. please note that this
                                           option will be replaced if max-
                                           partition-memory-bytes is also set
                                           (default: 16384)
--bootstrap-server <String: server to    REQUIRED unless --broker-list
  connect to>                              (deprecated) is specified. The server
                                           (s) to connect to. The broker list
                                           string in the form HOST1:PORT1,HOST2:
                                           PORT2.
--broker-list <String: broker-list>      DEPRECATED, use --bootstrap-server
                                           instead; ignored if --bootstrap-
                                           server is specified.  The broker
                                           list string in the form HOST1:PORT1,
                                           HOST2:PORT2.
--compression-codec [String:             The compression codec: either 'none',
  compression-codec]                       'gzip', 'snappy', 'lz4', or 'zstd'.
                                           If specified without value, then it
                                           defaults to 'gzip'
--help                                   Print usage information.
--line-reader <String: reader_class>     The class name of the class to use for
                                           reading lines from standard in. By
                                           default each line is read as a
                                           separate message. (default: kafka.
                                           tools.
                                           ConsoleProducer$LineMessageReader)
--max-block-ms <Long: max block on       The max time that the producer will
  send>                                    block for during a send request.
                                           (default: 60000)
--max-memory-bytes <Long: total memory   The total memory used by the producer
  in bytes>                                to buffer records waiting to be sent
                                           to the server. This is the option to
                                           control `buffer.memory` in producer
                                           configs. (default: 33554432)
--max-partition-memory-bytes <Integer:   The buffer size allocated for a
  memory in bytes per partition>           partition. When records are received
                                           which are smaller than this size the
                                           producer will attempt to
                                           optimistically group them together
                                           until this size is reached. This is
                                           the option to control `batch.size`
                                           in producer configs. (default: 16384)
--message-send-max-retries <Integer>     Brokers can fail receiving the message
                                           for multiple reasons, and being
                                           unavailable transiently is just one
                                           of them. This property specifies the
                                           number of retries before the
                                           producer give up and drop this
                                           message. This is the option to
                                           control `retries` in producer
                                           configs. (default: 3)
--metadata-expiry-ms <Long: metadata     The period of time in milliseconds
  expiration interval>                     after which we force a refresh of
                                           metadata even if we haven't seen any
                                           leadership changes. This is the
                                           option to control `metadata.max.age.
                                           ms` in producer configs. (default:
                                           300000)
--producer-property <String:             A mechanism to pass user-defined
  producer_prop>                           properties in the form key=value to
                                           the producer.
--producer.config <String: config file>  Producer config properties file. Note
                                           that [producer-property] takes
                                           precedence over this config.
--property <String: prop>                A mechanism to pass user-defined
                                           properties in the form key=value to
                                           the message reader. This allows
                                           custom configuration for a user-
                                           defined message reader.
                                         Default properties include:
                                          parse.key=false
                                          parse.headers=false
                                          ignore.error=false
                                          key.separator=\t
                                          headers.delimiter=\t
                                          headers.separator=,
                                          headers.key.separator=:
                                          null.marker=   When set, any fields
                                           (key, value and headers) equal to
                                           this will be replaced by null
                                         Default parsing pattern when:
                                          parse.headers=true and parse.key=true:
                                           "h1:v1,h2:v2...\tkey\tvalue"
                                          parse.key=true:
                                           "key\tvalue"
                                          parse.headers=true:
                                           "h1:v1,h2:v2...\tvalue"
--reader-config <String: config file>    Config properties file for the message
                                           reader. Note that [property] takes
                                           precedence over this config.
--request-required-acks <String:         The required `acks` of the producer
  request required acks>                   requests (default: -1)
--request-timeout-ms <Integer: request   The ack timeout of the producer
  timeout ms>                              requests. Value must be non-negative
                                           and non-zero. (default: 1500)
--retry-backoff-ms <Long>                Before each retry, the producer
                                           refreshes the metadata of relevant
                                           topics. Since leader election takes
                                           a bit of time, this property
                                           specifies the amount of time that
                                           the producer waits before refreshing
                                           the metadata. This is the option to
                                           control `retry.backoff.ms` in
                                           producer configs. (default: 100)
--socket-buffer-size <Integer: size>     The size of the tcp RECV size. This is
                                           the option to control `send.buffer.
                                           bytes` in producer configs.
                                           (default: 102400)
--sync                                   If set message send requests to the
                                           brokers are synchronously, one at a
                                           time as they arrive.
--timeout <Long: timeout_ms>             If set and the producer is running in
                                           asynchronous mode, this gives the
                                           maximum amount of time a message
                                           will queue awaiting sufficient batch
                                           size. The value is given in ms. This
                                           is the option to control `linger.ms`
                                           in producer configs. (default: 1000)
--topic <String: topic>                  REQUIRED: The topic id to produce
                                           messages to.
--version                                Display Kafka version.


./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic todos-topic
>Learn Kafka
[2023-09-21 16:53:27,327] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 4 : {todos-topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
>Learn Event Driven arch
>Learn Kafka Streams
.....................................................................................

What if the topic which is not present? Still can i publish events?

Yes we can publish events, first kafka will create new topic with default settings, after that events to be published.

./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic demo-topic
>This is demo
[2023-09-21 17:01:57,945] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 4 : {demo-topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
>This is another demo
>This sample topic

This is very usefull from the applications side, applications can create topics from the application program, then they publish events.
....................................................................................
			Consumers
....................................................................................

Consumers are going to read Events from the topics.
Consumers can read Events from the any where in the log file.
Consumers read events based on "Offset" - Offset is nothing but file postions

Steps:
1.Find a topic by name eg todos-topic
2.Find host and port eg Localhost:9092
3.if you want to read future messages(Current Message being published)
4.if you want to read histrical events from the begining.

./bin/kafka-console-consumer.sh --help

This tool helps to read data from Kafka topics and outputs it to standard output.
Option                                   Description
------                                   -----------
--bootstrap-server <String: server to    REQUIRED: The server(s) to connect to.
  connect to>
--consumer-property <String:             A mechanism to pass user-defined
  consumer_prop>                           properties in the form key=value to
                                           the consumer.
--consumer.config <String: config file>  Consumer config properties file. Note
                                           that [consumer-property] takes
                                           precedence over this config.
--enable-systest-events                  Log lifecycle events of the consumer
                                           in addition to logging consumed
                                           messages. (This is specific for
                                           system tests.)
--formatter <String: class>              The name of a class to use for
                                           formatting kafka messages for
                                           display. (default: kafka.tools.
                                           DefaultMessageFormatter)
--formatter-config <String: config       Config properties file to initialize
  file>                                    the message formatter. Note that
                                           [property] takes precedence over
                                           this config.
--from-beginning                         If the consumer does not already have
                                           an established offset to consume
                                           from, start with the earliest
                                           message present in the log rather
                                           than the latest message.
--group <String: consumer group id>      The consumer group id of the consumer.
--help                                   Print usage information.
--include <String: Java regex (String)>  Regular expression specifying list of
                                           topics to include for consumption.
--isolation-level <String>               Set to read_committed in order to
                                           filter out transactional messages
                                           which are not committed. Set to
                                           read_uncommitted to read all
                                           messages. (default: read_uncommitted)
--key-deserializer <String:
  deserializer for key>
--max-messages <Integer: num_messages>   The maximum number of messages to
                                           consume before exiting. If not set,
                                           consumption is continual.
--offset <String: consume offset>        The offset to consume from (a non-
                                           negative number), or 'earliest'
                                           which means from beginning, or
                                           'latest' which means from end
                                           (default: latest)
--partition <Integer: partition>         The partition to consume from.
                                           Consumption starts from the end of
                                           the partition unless '--offset' is
                                           specified.
--property <String: prop>                The properties to initialize the
                                           message formatter. Default
                                           properties include:
                                          print.timestamp=true|false
                                          print.key=true|false
                                          print.offset=true|false
                                          print.partition=true|false
                                          print.headers=true|false
                                          print.value=true|false
                                          key.separator=<key.separator>
                                          line.separator=<line.separator>
                                          headers.separator=<line.separator>
                                          null.literal=<null.literal>
                                          key.deserializer=<key.deserializer>
                                          value.deserializer=<value.
                                           deserializer>
                                          header.deserializer=<header.
                                           deserializer>
                                         Users can also pass in customized
                                           properties for their formatter; more
                                           specifically, users can pass in
                                           properties keyed with 'key.
                                           deserializer.', 'value.
                                           deserializer.' and 'headers.
                                           deserializer.' prefixes to configure
                                           their deserializers.
--skip-message-on-error                  If there is an error when processing a
                                           message, skip it instead of halt.
--timeout-ms <Integer: timeout_ms>       If specified, exit if no message is
                                           available for consumption for the
                                           specified interval.
--topic <String: topic>                  The topic to consume on.
--value-deserializer <String:
  deserializer for values>
--version                                Display Kafka version.
--whitelist <String: Java regex          DEPRECATED, use --include instead;
  (String)>                                ignored if --include specified.
                                           Regular expression specifying list
                                           of topics to include for consumption.


How to consume lastest events?

Lab:
Steps:
1.Start Producer and Publish events
./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic todos-topic
>Hello
>How are you
>how things are
>How kafka transfers messages
>How kafka uses messages/events

2.Start Consumer and list for events
./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic todos-topic
Hello
How are you
how things are
How kafka transfers messages
How kafka uses messages/events

How to consume messages from the begining and current messages?

./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic todos-topic --from-beginning
Learn Kafka
Learn Event Driven arch
Learn Kafka Streams
Hello
How are you
how things are
How kafka transfers messages
How kafka uses messages/events
Streams
.....................................................................................
			 Consumer Properties - TimeStamp
.....................................................................................

TimeStamp Property says that when event was published - Time of Event published

./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic todos-topic --from-beginning --property print.timestamp=true
CreateTime:1695295407429        Learn Kafka
CreateTime:1695295431899        Learn Event Driven arch
CreateTime:1695295448563        Learn Kafka Streams
CreateTime:1695296617270        Hello
CreateTime:1695296641027        How are you
CreateTime:1695296645434        how things are
CreateTime:1695296652725        How kafka transfers messages
CreateTime:1695296661746        How kafka uses messages/events
CreateTime:1695296890093        Streams
.....................................................................................
			   Topic Configuration-Log Rention
.....................................................................................

By Default , events (messages) are stored into Topics, topic finally persists into log file.

Can delete Events from the log file?
By default, No, Kafka log messages(events,Records) are immutable - read only. -

Can i delete log files?
  yes you can delete log file.

Deleting the logs which is called "Log Rention/Log Rention Policy"

Log Clean up Policies?
 
 In kafka , Unlike other messaging brokers, the messages/events on a topic are not immediately removed after they are consumed. Instead, the configurtion of each topic determines how much space the topic is permitted and how it is managed.

Concept of making data expire is called as "Clean Up". It is a topic level configuration.

Types of clean up policies:
...........................
		
			 Log Clean Up policies
				|
    -----------------------------------------------------
    |                    |             |
 
  DELETE              Compact         Compact and Delete



Delete Policy:
   It is default cleanup policy.This will discard old "segment".

.....................................................................................
		  "Over all Kafka is configuration based"

Based on configuration we can change the overall the behaviour of kafka either during start up or while running.

Types of configuration:
1.Static configuration
    Before starting kafka broker , configurations are set
2.Dynamic configuration
   After starting Kafka broker, configurations are passed while kafka broker in production.

Any Configuration can be setup in two levels

1.Broker level
   Setting up configurations for whole broker 
2.topic level
   You can setup configuration for particular topic, that configuration will not affect other configuratoin

Log Rention Policies can be setup via configuration either statically or dynmaically

Log Rention policies configuration can be setup "Broker level" or "Topic level"

if you set policy at broker level, which is applied to all topics which are created inside broker.


Log Rention Policy can be setup in two ways

1.Time Based 
2.Size based

1.Time Based: Period of Time
  I want to delete log files after 7 days, 7 months,7 weeks,7 years,7 mins, 7 secs ,7 mins

2.Size based
 I want to delete log files after certain size reached, for after reaching 1GB 
 The value must be postive no
 if you set -1, meaning that no limit on size.

Can i set both policies togther?

Yes, Which policy wins first, based on that policy, the log files will be deleted.


Time Based Configuration: Broker Level:

server.properties:
  This file contains all configurations related to broker

log.retention.hours

The number of hours to keep a log file before deleting it (in hours), tertiary to log.retention.ms property

Type:	int
Default:	168  - 7 days -  1 week
Valid Values:	
Importance:	high
Update Mode:	read-only

log.retention.minutes

The number of minutes to keep a log file before deleting it (in minutes), secondary to log.retention.ms property. If not set, the value in log.retention.hours is used

Type:	int
Default:	null
Valid Values:	
Importance:	high
Update Mode:	read-only

log.retention.ms
The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.

Type:	long
Default:	null
Valid Values:	
Importance:	high
Update Mode:	cluster-wide

Size Based Configuration:
........................

log.retention.bytes

The maximum size of the log before deleting it

Type:	long
Default:	-1
Valid Values:	
Importance:	high
Update Mode:	cluster-wide

log.retention.bytes=1073741824

When ever it reaches 1GB, log file will be deleted.
.....................................................................................
			 Topic Level Configuration
.....................................................................................

While creating a topic or after creating a topic we can set configuration related to Log rention policy.


--config <String: name=value>            A topic configuration override for the
                                           topic being created or altered. The
                                           following is a list of valid
                                           configurations:
                                                cleanup.policy
                                                compression.type
                                                delete.retention.ms
                                                file.delete.delay.ms
                                                flush.messages
                                                flush.ms
                                                follower.replication.throttled.
                                           replicas
                                                index.interval.bytes
                                                leader.replication.throttled.replicas
                                                local.retention.bytes
                                                local.retention.ms
                                                max.compaction.lag.ms
                                                max.message.bytes
                                                message.downconversion.enable
                                                message.format.version
                                                message.timestamp.difference.max.ms
                                                message.timestamp.type
                                                min.cleanable.dirty.ratio
                                                min.compaction.lag.ms
                                                min.insync.replicas
                                                preallocate
                                                remote.storage.enable
                                                retention.bytes
                                                retention.ms
                                                segment.bytes
                                                segment.index.bytes
                                                segment.jitter.ms
                                                segment.ms
                                                unclean.leader.election.enable
                                         See the Kafka documentation for full
                                           details on the topic configs. It is
                                           supported only in combination with --
                                           create if --bootstrap-server option
                                           is used (the kafka-configs CLI
                                           supports altering topic configs with
                                           a --bootstrap-server option).


Interval Property:
  The interval at which log segments are checked to see if they can be deleted according  to the retention policies.

log.retention.check.interval.ms=300000


.....................................................................................

Lab:

Steps:
1.Stop any running server
2.Open server.properties and change configurations
  log.retention.minutes=1
3.Start Kafka broker 
   Test after 1 min, and check along with

server log:
[2023-09-22 15:33:22,001] INFO [LocalLog partition=todos-topic-0, dir=/tmp/kafka-logs] Deleting segment files LogSegment(baseOffset=0, size=761, lastModifiedTime=1695296891088, largestRecordTimestamp=Some(1695296890093)) (kafka.log.LocalLog$)
[2023-09-22 15:33:22,003] INFO Deleted log /tmp/kafka-logs/todos-topic-0/00000000000000000000.log.deleted. (kafka.log.LogSegment)
[2023-09-22 15:33:22,006] INFO Deleted offset index /tmp/kafka-logs/todos-topic-0/00000000000000000000.index.deleted. (kafka.log.LogSegment)
[2023-09-22 15:33:22,006] INFO Deleted time index /tmp/kafka-logs/todos-topic-0/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment)
[2023-09-22 15:33:22,011] INFO [LocalLog partition=demo-topic-0, dir=/tmp/kafka-logs] Deleting segment files LogSegment(baseOffset=0, size=253, lastModifiedTime=1695295936788, largestRecordTimestamp=Some(1695295935795)) (kafka.log.LocalLog$)
[2023-09-22 15:33:22,013] INFO Deleted log /tmp/kafka-logs/demo-topic-0/00000000000000000000.log.deleted. (kafka.log.LogSegment)
[2023-09-22 15:33:22,014] INFO Deleted offset index /tmp/kafka-logs/demo-topic-0/00000000000000000000.index.deleted. (kafka.log.LogSegment)
[2023-09-22 15:33:22,015] INFO Deleted time index /tmp/kafka-logs/demo-topic-0/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment)
...................................................................................

Lab:
 Create topic with Rentension policy 

Steps:
1.create a topic with Retension policy

./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic demo-topic --config retention.ms=10000

Created topic hello-topic.

2.Produce some events
./bin/kafka-console-producer.sh -bootstrap-server localhost:9092 --topic hello-topic

3.consume events
/bin/kafka-console-consumer.sh -bootstrap-server localhost:9092 --topic hello-topic --from-beginning --property print.timestamp=true

Wait for 2 mins and check the messages again. You can notice that messages have been deleted(log file)

Lab:
 Based on Size 

log.retention.bytes=1073741824

You can enable this property some seetings like 1 kb

Steps: after enabling , restart the server and produce message until it reaches 1 kb once it reaches i kb you can note down that file would have been deleted.
....................................................................................
			  Dynamic Configuration
....................................................................................

kafka-configs.sh -To manage Runtime Configuration
  This tool helps to 
/bin/kafka-configs.sh --help
This tool helps to manipulate and describe entity config for a topic, client, user, broker or ip
Option                                 Description
------                                 -----------
--add-config <String>                  Key Value pairs of configs to add.
                                         Square brackets can be used to group
                                         values which contain commas: 'k1=v1,
                                         k2=[v1,v2,v2],k3=v3'. The following
                                         is a list of valid configurations:
                                         For entity-type 'topics':
                                        cleanup.policy
                                        compression.type
                                        delete.retention.ms
                                        file.delete.delay.ms
                                        flush.messages
                                        flush.ms
                                        follower.replication.throttled.
                                         replicas
                                        index.interval.bytes
                                        leader.replication.throttled.replicas
                                        local.retention.bytes
                                        local.retention.ms
                                        max.compaction.lag.ms
                                        max.message.bytes
                                        message.downconversion.enable
                                        message.format.version
....................................................................................

//Adding new config or editing existing configuration broker level setting.

Enity Types:
--entity-type <String>                 Type of entity
                                         (topics/clients/users/brokers/broker-
                                         loggers/ips)


./bin/kafka-configs.sh  --bootstrap-server localhost:9092 --alter --entity-type brokers --entity-default --add-config min.insync.replicas=2


 Here you are adding new configuration called "min.insync.replicas=2" to the existing broker configuration and mean time we have to keep existing configurations too.

If you are in client side ,like java programs

KafkaConfig myconfig=new KafkaConfig()
myconfig.setProperty("min.insync.replicas",2)

if you want delete existing configuration

./bin/kafka-configs.sh  --bootstrap-server localhost:9092 --alter --entity-type brokers --entity-default --delete-config min.insync.replicas

Topics Dynamic Configuration:

./bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --entity-type topics 
 --entity-default --entity-name configured-topic --add-config max.message.bytes=10232323


Lab:
Steps
1.create a new topic with default settings
./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic payment-topic
Created topic payment-topic.
2.Alter topic configuration like log rentention policy
/bin/kafka-configs.sh --bootstrap-server localhost:9092 --alter --entity-type topics --entity-name payment-topic --add-config retention.ms=10000
Completed updating config for topic payment-topic.
3.Describe the topic
/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic payment-topic
Topic: payment-topic    TopicId: ig7CcFjkQbKA7L1YiakRdQ PartitionCount: 1       ReplicationFactor: 1    Configs: retention.ms=10000,retention.bytes=1073741824
        Topic: payment-topic    Partition: 0    Leader: 0       Replicas: 0     Isr: 0
3.Produce message 
./bin/kafka-console-producer.sh -bootstrap-server localhost:9092 --topic payment-topic
>100 credited
>2000 debited
>3000 credited
>4000 debited
4.After producing some events , you can test whether log retention policy is applied 

Server log

[2023-09-22 17:22:29,746] INFO Deleted log /tmp/kafka-logs/payment-topic-0/00000000000000000000.log.deleted. (kafka.log.LogSegment)
[2023-09-22 17:22:29,747] INFO Deleted offset index /tmp/kafka-logs/payment-topic-0/00000000000000000000.index.deleted. (kafka.log.LogSegment)
[2023-09-22 17:22:29,747] INFO Deleted time index /tmp/kafka-logs/payment-topic-0/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment)
.....................................................................................
			       Partition
....................................................................................
What is Partition?
  In order to distribute the "storage and Process of Events" in a topic, Kafka uses the concept called "Partition"

The topic is broken into multiple partitions.

Every partition is just folder

Every Partition can have its own log files.

			Payment-topic
			   |
		----------------------------------------------
                |           |              |
         payment-topic-0  payment-topic-1 payment-topic-2 - folder
            |                    |            |
          log files          log files      logfiles  ----disk files

By default every topic has only one partition.

How to create multiple partitions?

./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic payment-topic --partitions 3

After supplying partition option you can see inside kafka-log folder

/tmp/kafka-logs
    |
    payment-topic-0
    payment-topic-1
    payment-topic-2

./bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic payment-topic 

./bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic payment-topic
Topic: payment-topic    TopicId: e7BoE8GFTcaxsskWMyI6OA PartitionCount: 3       ReplicationFactor: 1    Configs: retention.bytes=1073741824
        Topic: payment-topic    Partition: 0    Leader: 0       Replicas: 0     Isr: 0
        Topic: payment-topic    Partition: 1    Leader: 0       Replicas: 0     Isr: 0
        Topic: payment-topic    Partition: 2    Leader: 0       Replicas: 0     Isr: 0

Why Partitions?
 =>Partitions helps to distribute messages into multiple log files so that we dont need to dump into one single log file, we can scale the events across multiple files.
 =>Partitions are helping unit of parallelism.

.....................................................................................				  Segements
....................................................................................

What is segment?
  Segment is just actual log files containings "Records"

Topics are broken into partitions, and partitions are broken into segements.

Semgment means "log files"

How to list log files?

/tmp/kafka-logs/payment-topic-0$ ls -l
total 8
-rw-r--r-- 1 subu subu 10485760 Sep 25 14:33 00000000000000000000.index
-rw-r--r-- 1 subu subu        0 Sep 25 14:33 00000000000000000000.log
-rw-r--r-- 1 subu subu 10485756 Sep 25 14:33 00000000000000000000.timeindex
-rw-r--r-- 1 subu subu        8 Sep 25 14:33 leader-epoch-checkpoint
-rw-r--r-- 1 subu subu       43 Sep 25 14:33 partition.metadata

Here segement is group of files
.log
.index
.timeindex

00000000000000000000 -  Name of file.

Segment is just log file Which contains "Actual Records".

What is Record?
  Record is representation of "Event".
  Record is nothing but "Message"

Records are stored as "Byte Array"

00000000000000000000.log actuall stores "Records"
...................................................................................
			Segment File or Log File Architecture
....................................................................................

How events or data or message or record is stored segement?


Actual Log file is structured with two parts

1.Actual data-event
2.offset

As we discussed data/event is published into segement as "Record" which is simply byte array.

Offset:
An offset into a file is simply the character location within that file, usually starting with 0; thus "offset 240" is actually the 241st byte in the file.

  (Byte Array)
  a  b  c  d   e  f  g           Actual Record
.........................................
  0  1  2  3   4  5  6           Offset


When ever you publish event into topic, into partition and into segement event to be inserted along with offset no.

A Partition can have multiple segments meaning that i can have multiple log files.

Multiple log files inside single folder(Partition)

File Names

partion-0
  00000000000000000000.log  -Segement-0
  00000000000000000001.log  -Segement-1
  00000000000000000002.log  -Segement-2

Since We have multiple segement files inside a particular partitions. when we send message into that partition in which segement message will be inserted?

                 "Only Active Segement can receive Message"

How many segements a partitions can have?
 There is no limit no of "segement files"

When a Segement files are created?
   
   Segement files are created based on two conditions

1.Size
2.Time

The segement size(file size) is determined by the property

log.segement.bytes
   The maximum size of a single log file.
   When it reaches, kafka will create new one
   default is 1 gb

Time:
 log.segement.ms
   The Kafka will wait before creating new file based on time
  default is 1 week


How to represent multiple segement files and how messages are published into active segement?

Segement-0       Segement-1           Segement-2         
Offset 0 to 957  Offset 958 to 1500   Offset 1501 ---? <===writes
.....................................................................................

Lab: Mutliple segements
 In order to create new log files

server.properties
# The maximum size of a log segment file. When this size is reached a new log segment will be created.
#log.segment.bytes=1073741824

Steps:
1.Start Kakfa Server
2.create topic with single partitions (Only for testing, you can have more partitions)
3.Keep on publishing messages until it reaches 1000 bytes
4.Look into the log locations of /tmp/kafka-logs

/tmp/kafka-logs/todos-topic-0$ ls -l
total 24
-rw-r--r-- 1 subu subu        0 Sep 25 15:34 00000000000000000000.index
-rw-r--r-- 1 subu subu      986 Sep 25 15:34 00000000000000000000.log
-rw-r--r-- 1 subu subu       12 Sep 25 15:34 00000000000000000000.timeindex
-rw-r--r-- 1 subu subu 10485760 Sep 25 15:34 00000000000000000014.index
-rw-r--r-- 1 subu subu      784 Sep 25 15:34 00000000000000000014.log
-rw-r--r-- 1 subu subu       56 Sep 25 15:34 00000000000000000014.snapshot
-rw-r--r-- 1 subu subu 10485756 Sep 25 15:34 00000000000000000014.timeindex
-rw-r--r-- 1 subu subu        8 Sep 25 15:32 leader-epoch-checkpoint
-rw-r--r-- 1 subu subu       43 Sep 25 15:32 partition.metadata
....................................................................................
		                 Consumers
...................................................................................

Consumers can read Records from the "segment" based on "Offset only"

Consumers read Records

1.from beginining  - from the 0th offset
2.from the active  postion - from the current offset.
3.based on offset.

Note: if you read record based on offset , you must supply partition option.

 ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic todos-topic --offset 2 --partition 0
adfasfsaf
adfafadsfsaf
adfasfsadfasfd
dsafdsafdsafsadfdsafdsfsafdsafsadf
adfadfdsafdsafdsafdsafdsafdsafsadfdsfsadfsafsafsaf
adsfsafsafsdfsafsdfsadfasdfdsafadfafd
adfafsadf

fdfadf
adfsafd
adfasdfas
adsfsafsaf
adfafdasdf
adfadfadf
adfadfsafd
adfsfdasf
sgfsgsfg
sfsfgsdgf
sgsfgfdsg
sfsgdsfgsfdgfdsg
sgfsfgsgdsfg
sfgsgsgsfgsf
^CProcessed a total of 22 messages
subu@LAPTOP-R2TGGFDL:~/kafka_2.13-3.5.1$ ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic todos-topic --offset 0 --partition 0
adsfasdf
adfasfasf
adfasfsaf
adfafadsfsaf
adfasfsadfasfd
dsafdsafdsafsadfdsafdsfsafdsafsadf
adfadfdsafdsafdsafdsafdsafdsafsadfdsfsadfsafsafsaf
adsfsafsafsdfsafsdfsadfasdfdsafadfafd
adfafsadf

fdfadf
adfsafd
adfasdfas
adsfsafsaf
adfafdasdf
adfadfadf
adfadfsafd
adfsfdasf
sgfsgsfg
sfsfgsdgf
sgsfgfdsg
sfsgdsfgsfdgfdsg
sgfsfgsgdsfg
sfgsgsgsfgsf
^CProcessed a total of 24 messages
subu@LAPTOP-R2TGGFDL:~/kafka_2.13-3.5.1$ ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic todos-topic --offset 10 --partition 0
fdfadf
adfsafd
adfasdfas
adsfsafsaf
adfafdasdf
adfadfadf
adfadfsafd
adfsfdasf
sgfsgsfg
sfsfgsdgf
sgsfgfdsg
sfsgdsfgsfdgfdsg
sgfsfgsgdsfg
sfgsgsgsfgsf
^CProcessed a total of 14 messages
subu@LAPTOP-R2TGGFDL:~/kafka_2.13-3.5.1$
.....................................................................................
		 How to inspect the internals of log file
			(Dump log)
.....................................................................................

Some times when y are working with Kafka , you may find yourself needing to manually inspect underlying logs of a topic.

Whether you are just curious about Kafka internals or you need to debug an issue and verifthe content , the kafka-dump-log command is your friend.

kafka-dump-log.sh

Lab:
 Verify the internal of "Segement" file

/kafka_2.13-3.5.1$ ./bin/kafka-dump-log.sh --print-data-log --files /../tmp/kafka-logs/todos-topic-0/00000000000000000000.log

Dumping /../tmp/kafka-logs/todos-topic-0/00000000000000000000.log
Log starting offset: 0
baseOffset: 0 lastOffset: 1 count: 2 baseSequence: 0 lastSequence: 1 producerId: 0 producerEpoch: 0 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 0 CreateTime: 1695636244548 size: 93 magic: 2 compresscodec: none crc: 3980279278 isvalid: true
| offset: 0 CreateTime: 1695636243571 keySize: -1 valueSize: 8 sequence: 0 headerKeys: [] payload: adsfasdf
| offset: 1 CreateTime: 1695636244548 keySize: -1 valueSize: 9 sequence: 1 headerKeys: [] payload: adfasfasf
....................................................................................
			 Index files
.....................................................................................

.index
  Contains the mappings of "offset" to its position in ".log" file.
.timeindex
  file contains the mappings of "Timestamp" to message offset.

Messages(events) are searched by consumer based these index files only....

			 Topic
			  |
		..............................................
		|          |           |
	   partition-0   partition-1  partition-2
	     |
	  0000000000.log
            offset  0 1 2 3...
	    byte    a b c d .....
            |
         000000.index
          offset 0  1    2    3  ...
          byte   23 223  400  800
         000000.timeindex
          timestamp 2323232323    23232321212     23232323	
	  offsets       0            1             2            
...................................................................................
		Records(Events/Messages) Distributions and Partitions
...................................................................................

Topics are broken into partitions, partitions are brokents into segements

As a Producer, we send data to the Topic only, Topic only distributes messages among Partitions.

Partitioner:
............
   Partitioner is a process/alogorthim that will determine to which partition a sepcific message/record/event will be assigned

    "In Nutshell Partitioner is simple java Program/class having Routing 
     Algorthim"
            "That algorthim only decides where to go"

For eg I have a topic having 3 partition
  Record===>Publish====>Partitioner===> Where to go==> P0 OR P1 OR P2

Record contains information for how to select Partition.

Partitioner Algorthims:
......................
1.Round Robin Algorthim
2.Stick Partitioner Algorthim
3.Key Based Partitioner Alogorthim.

1.Round Robin Alogrthim.
  It is default Algorthim used in kafka olders versions less than 2.3
  lets i assume i have two partitions
  M1 ----> P1
  M2-----> P2
  M3------>P1
  M4 ----->P2

In Round robin messages are distributed equally to all partitions.

Drawbacks of Round Robin Algorthim:

1.if more messages and more partitions, there is possibility of higher latency and low performance.

2.Stick Partitioner Algorthim

 It is built on the top of Round Robin only.
 Sticky Partitioner wont send "a" single event/record rather it sends as a batch
 It improves the Performance,reduces network latency.
 This is default Partitioner in the lastest kafka.

Stick Partitioner = {Round Robin + Batching}

3.Key Based Partitioner:
........................

 In Kafka "Message/Event/Record" has Basic Structure is that "Records are organized or represented  as Key-Value" Pair.

Every Record must be organized as key value pair, but as of now we have not mentioned any where with key, then how to get keys.

Lab:
  How to print key of a record.

./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic todos-topic --offset 0  --partition 0 --property print.key=true
null    adsfasdf
null    adfasfasf
null    adfasfsaf
null    adfafadsfsaf
null    adfasfsadfasfd
null    dsafdsafdsafsadfdsafdsfsafdsafsadf
null    adfadfdsafdsafdsafdsafdsafdsafsadfdsfsadfsafsafsaf
null    adsfsafsafsdfsafsdfsadfasdfdsafadfafd
null    adfafsadf
null
null    fdfadf
null    adfsafd
null    adfasdfas
null    adsfsafsaf
null    adfafdasdf
null    adfadfadf
null    adfadfsafd
null    adfsfdasf
null    sgfsgsfg
null    sfsfgsdgf
null    sgsfgfdsg
null    sfsgdsfgsfdgfdsg
null    sgfsfgsgdsfg
null    sfgsgsgsfgsf


Here "null" is Key "sfgsgsgsfgsf" is value

By default every message has "null" key

	"If you produce record/message/event without key means with null key"

        "If Record has null key, Kafka selects  Sticky Partitioner by default"
....................................................................................

Key based on Distribution:
..........................

Lab:
1.create topic with 2 partitions
2.Publish message with explicit Key
3.Consume message

Publish Message with Key:

--property "parse.key=true" --property "key-separator=:"

Steps:
1. ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic products-topic --partitions 2

2../bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic products-topic --property "parse.key=true" --property "key.separator=:"
>id:1
>name:Subramaian
>id:2
>name:Ram
>id:1
>name:Karthik
>hello
org.apache.kafka.common.KafkaException: No key separator found on line number 7: 'hello'
        at kafka.tools.ConsoleProducer$LineMessageReader.kafka$tools$ConsoleProducer$LineMessageReader$$parse(ConsoleProducer.scala:437)
        at kafka.tools.ConsoleProducer$LineMessageReader$$anon$3.hasNext(ConsoleProducer.scala:407)
        at kafka.tools.ConsoleProducer$.loopReader(ConsoleProducer.scala:90)
        at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:99)
        at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)

Note : if you insert any record without key, it will throw error.

3. ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic products-topic --property print.key=true --property  print.value=true --property print.partition=true --from-beginning

Partition:1     id      1
Partition:0     name    Subramaian
Partition:1     id      2
Partition:0     name    Ram
Partition:1     id      1
Partition:0     name    Karthik
Partition:1     hello   test

How key distribution is happening internally?

In old Kafka , they used "round Robin"

Kafka later introduced  an alogorthim called 'key' - 'Key hashing Algorthim'

Key hashing alogrthim uses the process of determining the mapping of a key to a parttion in the default partitioner.
The keys are hashed based on alogrthim called "murmur2"

  Target Partition = Math.abs(Util.murmur2(keyInBytes) % (numOfPartions-1)

Note:
 if you have same key,but different values, the same Partition will be used.
..................................................................................
			 Replication
..................................................................................

How to enable replication?
  We can enable replication at topic level. when a new topic is created we can specifiy exactly or through default, how many replicas we want.

Lab:
Start All servers

./bin/zookeeper-server-start.sh config/zookeeper.properties
 ./bin/kafka-server-start.sh config/server.properties

create topic with replication factor:
 --replication-factor number


/bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic product-topic --create --partitions 2 --replication-factor 3

Error while executing topic command : Replication factor: 3 larger than available brokers: 1.
[2023-09-27 15:01:49,692] ERROR org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 3 larger than available brokers: 1.
 (kafka.admin.TopicCommand$)

Why we are getting this error?
  Because we are asking topic to be replicated into 3 servers but we have only one server is running.

Before setting up replicas, we are going to use "Kraft"
.....................................................................................
			KRaft Mode
.....................................................................................	
In KRaft Mode, we dont need zookeeper.

Types of Brokers

1.Controller 
    It is broker Which manages meta data management,leader election,notification to other brokers
2.Broker
   It is holding data plane - Getting request from producer,and consumer, replication

3.Controller  and Broker  
   It can act as controller and broker
		
Lab:
   Single kafka broker with Single Controller, same server is going to act as broker and controller

Configuration:
 inside config folder there is folder called "KRaft"

kafka Distribution provides template configuration properties 

config/kraft/
  broker.properties  --> You can use this file to start as "broker-data plane"
  controller.properties - You can use this file to start as "controller-Control plane"
  server.properties - You can use this file to start "both broker and controller"
 

Steps:
1.Generate a Cluster ID (note : This is step needed for KRaft Mode)

 Before generating cluster id, we need to know, how to use Kafka-storage.sh utility

./bin/kafka-storage.sh --help
usage: kafka-storage [-h] {info,format,random-uuid} ...

The Kafka storage tool.

positional arguments:
  {info,format,random-uuid}
    info                 Get information about the Kafka log directories on this node.
    format               Format the Kafka log directories on this node.
    random-uuid          Print a random UUID.

optional arguments:
  -h, --help             show this help message and exit


1.Cluster ID declaration:

KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
echo $KAFKA_CLUSTER_ID
z5BC8G9GSjinmRy27PS1FQ

2.Format log directories using cluster id
./bin/kafka-storage.sh format -t  $KAFKA_CLUSTER_ID -c config/kraft/server.properties

Formatting /tmp/kraft-combined-logs with metadata.version 3.5-IV2.

3.Start Server In KRaft mode
./bin/kafka-server-start.sh config/kraft/server.properties

4.Create Simple topic and test
./bin/kafka-topics.sh --create --topic todos-topic --bootstrap-server localhost:9092

Created topic todos-topic.

5.Describing
./bin/kafka-topics.sh --describe --topic todos-topic --bootstrap-server localhost:90
92

Topic: todos-topic    TopicId: iLRR7RdYSgy-kWYK1Z48uQ PartitionCount: 1       ReplicationFactor: 1    Configs: segment.bytes=1073741824
Topic: todos-topic      Partition: 0    Leader: 1       Replicas: 1     Isr: 1
....................................................................................
			  Multi node setup with Replication
.....................................................................................

Steps:

1.Just Explore config files

config/kraft/
  broker.properties  --> You can use this file to start as "broker-data plane"
  controller.properties - You can use this file to start as "controller-Control   plane"
  server.properties - You can use this file to start "both broker and controller"
 
broker.properties =  only Broker-Data plane
controller.properties - Only Controll Plane
server.properties - Both Data plan + Control Plan

Exploring Config properties:
............................
broker.properties

############################# Server Basics #############################
# The role of this server. Setting this puts us in KRaft mode
process.roles=broker


controller.properties

############################# Server Basics #############################
# The role of this server. Setting this puts us in KRaft mode
process.roles=controller

server.properties

############################# Server Basics #############################

# The role of this server. Setting this puts us in KRaft mode
process.roles=broker,controller

KRaft Mode:

 ./bin/kafka-server.sh  config/kraft/broker.properties
 ./bin/kafka-server.sh  config/kraft/controller.properties
 ./bin/kafka-server.sh  config/kraft/server.properties

Zookeeper Mode:
  
 ./bin/kafka-server.sh  config/server.properties

.....................................................................................
			 Multi node Architecture
....................................................................................

We need to start Kafka brokers, How each brokers are identified?
 Using Broker id / node id
node.id=1 node.id=2 node.id=3 node.id=3
..................................................................................
			 Controller Configuration
.................................................................................

You can configure list of controllers in the cluster

controller.quorum.voters=1@localhost:9093

This configuration is for identifying other controllers in the cluster.

Here 
  "1" - is node Id => This node one is controller
  "localhost" - is host nme of the controller
  "9093" is port on the which controller is running.

Let us say i have three nodes

node-1
node-2
node-3

node-1 is designated as broker
node-2 is desginated as controller
node-3 is desginated as controller

node-1
 process.roles=broker
 node.id=1

node-2
 process.roles=controller
 node.id=2
 controller.quorum.voters=2@localhost:9092,3@localhost:9093


node-3
 process.roles=controller
 node.id=3
 controller.quorum.voters=2@localhost:9092,3@localhost:9093
.....................................................................................

Both Controller and Broker 

node-1
node-2
node-3

node-1 is designated as broker and controller
node-2 is desginated as controller and broker
node-3 is desginated as controller and broker

node-1
 process.roles=broker,controller
 node.id=1
 controller.quorum.voters=1@localhost:9091,2@localhost:9092,3@localhost:9093

node-2
 process.roles=controller,broker
 node.id=2
 controller.quorum.voters=1@localhost:9091,2@localhost:9092,3@localhost:9093


node-3
 process.roles=controller,broker
 node.id=3
 controller.quorum.voters=1@localhost:9091,2@localhost:9092,3@localhost:9093
.......................................................................
			Socket Server Settings
............................................................................

broker.properties

listeners = listener_name://host_name:port

This syntax is specific to URI scheme(rule) syntax  
eg:         
http://localhost:8080
ws://lococalhost:8080
jdbc:mysql://localhost:3306

      listener_name://host_name:port
	  |
       protocal://host_name:port

listeners=PLAINTEXT://localhost:9092

Types of Kafka Protocals:
........................

1.PLAINTEXT
	This is protocal used for brokers communications

# Name of listener used for communication between brokers.
inter.broker.listener.name=PLAINTEXT

2.CONTROLLER 
   This is protocal used for controller communication.

# A comma-separated list of the names of the listeners used by the controller.
# This is required if running in KRaft mode. On a node with `process.roles=broker`, only the first listed listener will be used by the broker.
controller.listener.names=CONTROLLER
..................................................................................

Protocal Configurations In each config files:

controller.properties

listeners=CONTROLLER://:9093
 
>Controller is running in the 9093 port and other Controller communicate via "Controller Protocal"

broker.properties

listeners=PLAINTEXT://localhost:9092
 
>Broker is running in the 9092 port and other Brokers communicate via "PLAINTEXT" protocal

server.properties
listeners=PLAINTEXT://:9092,CONTROLLER://:9093

Broker is running in 9092 and other brokers communicate via "PLAINTEXT" Protcal
Controller is running in 9093 and other controllers communciate via "CONTROLLER" Protocal
.....................................................................................
			advertised.listeners
.....................................................................................

In Kafka cluster who will be communicated by "Producers and Consumers"?

Only Brokers not Controllers

How producers and Consumers are communicating?

 They also need Protocal to commmunicate.

"PLAINTEXT" is default Protocal used by Clients

advertised.listeners=PLAINTEXT://localhost:9092

.....................................................................................
			    LOG Dirs
.....................................................................................
if you set up multi nodes, we need to configure for each node we need separate log dirs.

if you run brokers and controllers in the different host(machines), we dont need to configure separate folders

if you run brokers and controllers in the same host(machines), we  need to configure separate folders

log.dirs=/tmp/server-1/kraft-coimbined-logs
log.dirs=/tmp/server-2/kraft-coimbined-logs
log.dirs=/tmp/server-3/kraft-coimbined-logs
....................................................................................

Lab: How many servers you are going to create those many server config files can be 
created

I am going to create 3 config files: 

Note : I am going to setup both broker and controller 
/kafka_2.13-3.5.1/config/kraft$ cp server.properties server1.properties
/kafka_2.13-3.5.1/config/kraft$ cp server.properties server2.properties
/kafka_2.13-3.5.1/config/kraft$ cp server.properties server3.properties


Node-1 Setup: server1.properties

Controller port and broker port should be different.

process.roles=broker,controller
node.id=1
controller.quorum.voters=1@localhost:19092,2@localhost:19093,3@localhost:19094


listeners=PLAINTEXT://:9092,CONTROLLER://:19092
inter.broker.listener.name=PLAINTEXT
advertised.listeners=PLAINTEXT://localhost:9092
controller.listener.names=CONTROLLER

log.dirs=/tmp/server1/kraft-combined-logs

.................
Node-2 Setup: server2.properties

process.roles=broker,controller
node.id=2
controller.quorum.voters=1@localhost:19092,2@localhost:19093,3@localhost:19094

listeners=PLAINTEXT://:9093,CONTROLLER://:19093
inter.broker.listener.name=PLAINTEXT
advertised.listeners=PLAINTEXT://localhost:9093
controller.listener.names=CONTROLLER

log.dirs=/tmp/server2/kraft-combined-logs

.........................................
Node-3 Setup: server3.properties

process.roles=broker,controller
node.id=3
controller.quorum.voters=1@localhost:19092,2@localhost:19093,3@localhost:19094

listeners=PLAINTEXT://:9094,CONTROLLER://:19094
inter.broker.listener.name=PLAINTEXT
advertised.listeners=PLAINTEXT://localhost:9094
controller.listener.names=CONTROLLER
log.dirs=/tmp/server3/kraft-combined-logs
....................................................................................
Step 4 : Generate ClusterId:
.............................
./bin/kafka-storage.sh random-uuid
  fWlzC68kRUOfsEFpaFnEtw

Step 5: Format all storage directories
subu@LAPTOP-R2TGGFDL:~/kafka_2.13-3.5.1$ ./bin/kafka-storage.sh format -t fWlzC68kRUOfsEFpaFnEtw -c ./config/kraft/server1.properties
Formatting /tmp/server1/kraft-combined-logs with metadata.version 3.5-IV2.
subu@LAPTOP-R2TGGFDL:~/kafka_2.13-3.5.1$ ./bin/kafka-storage.sh format -t fWlzC68kRUOfsEFpaFnEtw -c ./config/kraft/server2.properties
Formatting /tmp/server2/kraft-combined-logs with metadata.version 3.5-IV2.
subu@LAPTOP-R2TGGFDL:~/kafka_2.13-3.5.1$ ./bin/kafka-storage.sh format -t fWlzC68kRUOfsEFpaFnEtw -c ./config/kraft/server3.properties
Formatting /tmp/server3/kraft-combined-logs with metadata.version 3.5-IV2.
s

Step 5:
 We need to allocat max memory, in order to work cluster properly.

export KAFKA_HEAP_OPTS="-Xmx200M -Xms100M"

Step 6: 
  We need to start all kafka servers in daemon mode / log mode

Log Mode:

./bin/kafka-server-start.sh ./config/kraft/server1.properties
./bin/kafka-server-start.sh ./config/kraft/server1.properties
./bin/kafka-server-start.sh ./config/kraft/server1.properties

Daemon Mode
./bin/kafka-server-start.sh -daemon ./config/kraft/server1.properties
./bin/kafka-server-start.sh -daemon ./config/kraft/server1.properties
./bin/kafka-server-start.sh -daemon ./config/kraft/server1.properties


Step 7:
 Test all Kafka servers are running
jcmd |grep kafka
7864 kafka.Kafka ./config/kraft/server3.properties
8329 kafka.Kafka ./config/kraft/server1.properties
7434 kafka.Kafka ./config/kraft/server2.properties

Step 8:
 create topic
 ./bin/kafka-topics.sh --create --topic kraft-test --partitions 3 --replication-factor 3 --bootstrap-ser
ver localhost:9092
Created topic kraft-test.

Step 9:
 Describe
./bin/kafka-topics.sh --describe --topic kraft-test  --bootstrap-server localhost:9092

Topic: kraft-test     TopicId: XMWG3b5rTQavatJXlQFFHg
PartitionCount: 3     ReplicationFactor: 3    Configs: segment.bytes=1073741824

Topic: kraft-test       Partition: 0    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1
Topic: kraft-test       Partition: 1    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2
Topic: kraft-test       Partition: 2    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3

Kafka is replicating "log files" , log files are distributed among partitions.

payment-topic-0 - 00000000000.log ----->

Here for partition 0 leader is 2 (node-id=2)  Replicas: 2,3,1 Isr: 2,3,1

Step 10:

/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic kraft-test
>Hello

./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kraft-test
Hello

Verify each server logs whether data has been replicated or not.
....................................................................................
				Kafka Connect
.....................................................................................

Kafka provides

1.Storage layer
  - How to store events
  -Topic partitions segmentation
  -Replication
  -Management
2.Compute layer
   How to communicate Kafka stroage layer
   How to process kafa data.

1.producers
2.consumers
.....................................................................................
What is Kafka connect?

Kafka connect is a tool for scalably and reliably streaming data between apache kafka and other data systems.
It makes it simple to quickly define connectors that move large set of data in and out of kafka.

Connectors Core Concept:

1.connect
   It is server, which performs read and write data from the data sources into kafka storage layer called "broker"	

2.connector
   It is java lib/jar file which provides ligh level abstraction that takes care of data transfers

3.Task
    The implmenetation of how data is copied to or from kafka

4.Workers
    The workers or connect are running process that execute connectors and taks

5.Converters
   Data Serialization and deserialization tasks

Why Connectors?

 Data-centric piplelines: Connect uses meaningull data abstractions to pull or push data to kafka and from kafka.

Types of connectors:

1.Source connector
   Collect data and send data into kafka storage layer.
2.Sink Connector
   Collect data from the Kafka storage layer and send to another data source
....................................................................................
			Connector implementation
....................................................................................

There are basic steps:

1.start zookeeper if you want , or with Kraft mode.
2.Connect Server 
3.Data Source - it can be any data source

Connect Server modes:
1.standalone mode- single server
2.distributed mode -multi node server like multi broker


Config files for connect Server (Apache Kafka):
................................................

config/connect-standalone.properties

Basic properties:
.................
bootstrap.servers=localhost:9092
  The connect server is going to connect which kafka broker

key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
  
  The connect will convert into incoming and outgoing events and data into json


Plugins:
  Connectors jar files / list of jar files

1.connect jar files
2.any dependency jar files

plugin.path=libs/connect-file-3.5.1.jar

  Connect server looks the plugin.path to activate "Connectors"

Connectors:
connect-file-3.5.1.jar -To reda data from the disk file and transfer data into kafka topic
.....................................................................................
			  UseCase :  File to Kafka 
....................................................................................

Are you going to use Source Connector or Sink Connector.

if you are source connector

config/connect-file-source.properties

name=local-file-source
connector.class=FileStreamSource
tasks.max=1
file=/home/subu/kafka_2.13-3.5.1/test.txt
topic=connect-test

if you are using sink connector
config/connect-file-sink.properties

name=local-file-sink
connector.class=FileStreamSink
tasks.max=1
file=/home/subu/kafka_2.13-3.5.1/test.sink.txt
topics=connect-test


Note: To get the current location 
$pwd
/home/subu/kafka_2.13-3.5.1

Steps:
1.create test.txt file and keep some data
 /kafka_2.13-3.5.1$ echo -e "hello" > test.txt

>File would have been created in the root kafka folder.


2.Start Zookeeper or Kraft
./bin/zookeeper-server-start.sh config/zookeeper.properties
3.Start Broker
 ./bin/kafka-server-start.sh config/server.properties
4.Start Connect
./bin/connect-standalone.sh config/connect-standalone.properties  config/connect-file-source.properties  config/connect-file-sink.properties

5.Start Consumer to test data arrival.
./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning

{"schema":{"type":"string","optional":false},"payload":"hello"}
{"schema":{"type":"string","optional":false},"payload":"How are you"}
{"schema":{"type":"string","optional":false},"payload":"i am"}

6.Add More data into file and see changes inthe console
kafka_2.13-3.5.1$ echo i am >> test.txt
...................................................................................
				Confulent Platform
..................................................................................

What is Confluent?

 Confluent is basically a company who gives enterprise "Kafka Platform" -Confluent Kafka
 Confluent Kafka is abstraction on the top of Apache kafka.

 Confluent Kafka provides various production ready features.

There are two major deployments

1.Cloud
2.Self Managed

Self Managed

Local:
 zip -window
 tar -Linux
 docker

Distributed
 -Kubernets 
 -Ansible Playbooks

Steps:

curl -O https://packages.confluent.io/archive/7.5/confluent-7.5.0.tar.gz

tar xzf confluent-7.5.0.tar.gz.tar

Folder description:

/bin/  Driver scripts for starting and stopping services

/etc/  Configuration files 

/lib/ systemd services

/libexec Multi platform cli binaries

/share/  jars and some licenses

/src/ Source files that requires a Platform independent build

How to start using confluent ?

./bin/confluent --help
Manage your Confluent Cloud or Confluent Platform. Log in to see all available commands.

Usage:
  confluent [command]

Available Commands:
  cloud-signup    Sign up for Confluent Cloud.
  completion      Print shell completion code.
  context         Manage CLI configuration contexts.
  help            Help about any command
  kafka           Manage Apache Kafka.
  local           Manage a local Confluent Platform development environment.
  login           Log in to Confluent Cloud or Confluent Platform.
  logout          Log out of Confluent Cloud or Confluent Platform.
  plugin          Manage Confluent plugins.
  prompt          Add Confluent CLI context to your terminal prompt.
  secret          Manage secrets for Confluent Platform.
  shell           Start an interactive shell.
  version         Show version of the Confluent CLI.

Flags:
      --version         Show version of the Confluent CLI.
  -h, --help            Show help for this command.
      --unsafe-trace    Equivalent to -vvvv, but also log HTTP requests and responses which may contain plaintext secrets.
  -v, --verbose count   Increase verbosity (-v for warn, -vv for info, -vvv for debug, -vvvv for trace).

Use "confluent [command] --help" for more information about a command.

In order to use Confluent in simple way 
....................................................................................
Optionally configure CONFLUENT_HOME

export CONFLUENT_HOME=<The directory where Confluent is installed>

export PATH=$PATH:$CONFLUENT_HOME/bin

eg:
$pwd
/home/subu

export CONFLUENT_HOME=/home/subu/confluent-7.5.0
export PATH=$PATH:$CONFLUENT_HOME/bin
.................

How to use confluent commands?
confluent --help
Manage your Confluent Cloud or Confluent Platform. Log in to see all available commands.

Usage:
  confluent [command]

Available Commands:
  cloud-signup    Sign up for Confluent Cloud.
  completion      Print shell completion code.
  context         Manage CLI configuration contexts.
  help            Help about any command
  kafka           Manage Apache Kafka.
  local           Manage a local Confluent Platform development environment.
  login           Log in to Confluent Cloud or Confluent Platform.
  logout          Log out of Confluent Cloud or Confluent Platform.
  plugin          Manage Confluent plugins.
  prompt          Add Confluent CLI context to your terminal prompt.
  secret          Manage secrets for Confluent Platform.
  shell           Start an interactive shell.
  version         Show version of the Confluent CLI.

Flags:
      --version         Show version of the Confluent CLI.
  -h, --help            Show help for this command.
      --unsafe-trace    Equivalent to -vvvv, but also log HTTP requests and responses which may contain plaintext secrets.
  -v, --verbose count   Increase verbosity (-v for warn, -vv for info, -vvv for debug, -vvvv for trace).

Use "confluent [command] --help" for more information about a command.
..............

confluent local --help
Use the "confluent local" commands to try out Confluent Platform by running a single-node instance locally on your machine. Keep in mind, these commands require Java to run.

Usage:
  confluent local [command]

Available Commands:
  current     Get the path of the current Confluent run.
  destroy     Delete the data and logs for the current Confluent run.
  services    Manage Confluent Platform services.
  version     Print the Confluent Platform version.

Global Flags:
  -h, --help            Show help for this command.
      --unsafe-trace    Equivalent to -vvvv, but also log HTTP requests and responses which may contain plaintext secrets.
  -v, --verbose count   Increase verbosity (-v for warn, -vv for info, -vvv for debug, -vvvv for trace).

Use "confluent local [command] --help" for more information about a command.
...

confluent local services --help
Manage Confluent Platform services.

Usage:
  confluent local services [command]

Available Commands:
  connect         Manage Connect.
  control-center  Manage Control Center.
  kafka           Manage Apache Kafka®.
  kafka-rest      Manage Kafka REST.
  ksql-server     Manage ksqlDB Server.
  list            List all Confluent Platform services.
  schema-registry Manage Schema Registry.
  start           Start all Confluent Platform services.
  status          Check the status of all Confluent Platform services.
  stop            Stop all Confluent Platform services.
  top             View resource usage for all Confluent Platform services.
  zookeeper       Manage Apache ZooKeeper™.

Global Flags:
  -h, --help            Show help for this command.
      --unsafe-trace    Equivalent to -vvvv, but also log HTTP requests and responses which may contain plaintext secrets.
  -v, --verbose count   Increase verbosity (-v for warn, -vv for info, -vvv for debug, -vvvv for trace).

Use "confluent local services [command] --help" for more information about a command.

...................................................................................
			 How to start confluent platform services

1.zookeeper
2.broker
3.control center
4.rest api server
5.streaming server
6.connect server
 confluent local services start
The local commands are intended for a single-node development environment only, NOT for production usage. See more: https://docs.confluent.io/current/cli/index.html
As of Confluent Platform 8.0, Java 8 is no longer supported.

Using CONFLUENT_CURRENT: /tmp/confluent.960781
Starting ZooKeeper
ZooKeeper is [UP]
Starting Kafka
Kafka is [UP]
Starting Schema Registry
Schema Registry is [UP]
Starting Kafka REST
Kafka REST is [UP]
Starting Connect
Connect is [UP]
Starting ksqlDB Server
ksqlDB Server is [UP]
Starting Control Center
....................................................................................
How to use confluent platform

There are two ways 
1.gui tool - control center
2.using cli tools

Task:
 Lanuch Center
 Create topic 
 Publish Some message
 
export CONFLUENT_HOME=/home/subu/confluent-7.5.0
 kafka-console-consumer --bootstrap-server localhost:9092 --topic todos_topic --from-beginning
{"ordertime":1497014222380,"orderid":18,"itemid":"Item_184","address":{"city":"Mountain View","state":"CA","zipcode":94041}}
{"ordertime":1497014222380,"orderid":18,"itemid":"Item_184","address":{"city":"Mountain View","state":"CA","zipcode":94041}}
{"ordertime":1497014222380,"orderid":18,"itemid":"Item_184","address":{"city":"Mountain View","state":"CA","zipcode":94041}}
{"ordertime":1497014222380,"orderid":18,"itemid":"Item_184","address":{"city":"Mountain View","state":"CA","zipcode":94041}}
{"ordertime":1497014222380,"orderid":18,"itemid":"Item_184","address":{"city":"Mountain View","state":"CA","zipcode":94041}}
{"ordertime":1497014222380,"orderid":18,"itemid":"Item_184","address":{"city":"Mountain View","state":"CA","zipcode":94041}}
{"ordertime":1497014222380,"orderid":18,"itemid":"Item_184","address":{"city":"Mountain View","state":"CA","zipcode":94041}
.....................................................................................				Confluent Platform and connectors
.....................................................................................
JDBC Connectors

1.JDBC Connectors
2.JBDC Sink Connectors

Most of the confluent connectors are available
https://www.confluent.io/hub/

What is jdbc connector?
 jdbc connector is just jar file

in order to install jdbc connector into confluent platform we have two ways

1.Manuall installation
2.via confluent hub client cli tool.

Steps:
 in order to with jdbc connectors 
you need to what database you are going to use (mysql,postresql,mssql,oracle...)

1.Make sure that database is running
2.Jdbc driver jars
3.Jdbc connector jars.

CONNECTOR HUB URL:
https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc

export CONFLUENT_HOME=/home/subu/confluent-7.5.0
export PATH=$PATH:$CONFLUENT_HOME/bin
confluent-hub install confluentinc/kafka-connect-jdbc:10.7.4
The component can be installed in any of the following Confluent Platform installations:
  1. /home/subu/confluent-7.5.0 (based on $CONFLUENT_HOME)
  2. /home/subu/confluent-7.5.0 (found in the current directory)
  3. /home/subu/confluent-7.5.0 (where this tool is installed)
Choose one of these to continue the installation (1-3): 1
Do you want to install this into /home/subu/confluent-7.5.0/share/confluent-hub-components? (yN) y


Component's license:
Confluent Community License
https://www.confluent.io/confluent-community-license
I agree to the software license agreement (yN) y

Downloading component Kafka Connect JDBC 10.7.4, provided by Confluent, Inc. from Confluent Hub and installing into /home/subu/confluent-7.5.0/share/confluent-hub-components
Detected Worker's configs:
  1. Standard: /home/subu/confluent-7.5.0/etc/kafka/connect-distributed.properties
  2. Standard: /home/subu/confluent-7.5.0/etc/kafka/connect-standalone.properties
  3. Standard: /home/subu/confluent-7.5.0/etc/schema-registry/connect-avro-distributed.properties
  4. Standard: /home/subu/confluent-7.5.0/etc/schema-registry/connect-avro-standalone.properties
  5. Based on CONFLUENT_CURRENT: /tmp/confluent.960781/connect/connect.properties
  6. Used by Connect process with PID 17511: /tmp/confluent.960781/connect/connect.properties
Do you want to update all detected configs? (yN) y

Adding installation directory to plugin path in the following files:
  /home/subu/confluent-7.5.0/etc/kafka/connect-distributed.properties
  /home/subu/confluent-7.5.0/etc/kafka/connect-standalone.properties
  /home/subu/confluent-7.5.0/etc/schema-registry/connect-avro-distributed.properties
  /home/subu/confluent-7.5.0/etc/schema-registry/connect-avro-standalone.properties
  /tmp/confluent.960781/connect/connect.properties
  /tmp/confluent.960781/connect/connect.properties

Completed

If you are going to use "mysql" database, mysql connector jar must have been placed 

home\subu\confluent-7.5.0\share\confluent-hub-components\confluentinc-kafka-connect-jdbc\lib
mysql-connector-java-8.0.16.jar

After installing make sure that etc/kafka/server.properties files having the below configuration.

By default confluent platform kafka listner configuration is disabled by default , we need to enable

listeners=PLAINTEXT://:9092

# Listener name, hostname and port the broker will advertise to clients.
# If not set, it uses the value for "listeners".
advertised.listeners=PLAINTEXT://your.host.name:9092

Make sure that kafka-connect-jdbc-10.7.4.jar is also present in that lib location.
..............................

Maksure that etc/kafka/connect-standalone.properties

plugin.path=/home/subu/confluent-7.5.0/share/confluent-hub-components
....................................................................................

Start Database:
  ->Mysql database via docker.

Start mysql server:
 This mysql server contains ready made database called inventory.

docker run -it --rm --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw quay.io/debezium/example-mysql:2.3

start Mysql client utilty
 docker run -it --rm --name mysqlterm --link mysql --rm mysql:8.0 sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'












